{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ankile_MIT6.036 Hw 08 Colab Notebook",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankile/MIT6.036-Intro-Machine-Learning/blob/master/hw-08/ankile_MIT6_036_Hw_08_Colab_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIaEwCD406A",
        "colab_type": "text"
      },
      "source": [
        "#MIT 6.036 Fall 2020: Homework 8#\n",
        "\n",
        "This colab notebook provides code and a framework for [homework 8](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week8/week8_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-S-l98HBVoa",
        "colab_type": "code",
        "outputId": "86c5313f-ba1a-4a61-c0fd-21f62dfa6477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "!rm -rf code_for_hw8*\n",
        "!rm -rf data\n",
        "!rm -rf mnist_data\n",
        "!rm -rf *.zip\n",
        "!rm -rf test*/\n",
        "!rm -rf *.py\n",
        "!rm -rf *.pt\n",
        "!rm -rf __*\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw08/code_for_hw8.zip\n",
        "!unzip code_for_hw8.zip\n",
        "!unzip code_for_hw8/q4.zip\n",
        "!unzip -q test1.zip\n",
        "!unzip -q test2.zip\n",
        "!unzip -q test3.zip\n",
        "!mv code_for_hw8/* .\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "import math as m\n",
        "\n",
        "import torch\n",
        "from torch.nn import (Linear, ReLU, Conv1d, Flatten, Conv2d, Sequential, \n",
        "                      MaxPool1d, MaxPool2d, Dropout, CrossEntropyLoss, MSELoss)\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision\n",
        "\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "\n",
        "import os\n",
        "\n",
        "from code_for_hw8_pytorch import get_image_data_1d\n",
        "\n",
        "from utils_hw8 import (model_fit, model_evaluate, run_pytorch, call_model, \n",
        "                       plot_decision, plot_heat, plot_separator, make_iter, \n",
        "                       set_weights, set_bias)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  code_for_hw8.zip\n",
            "   creating: code_for_hw8/\n",
            "  inflating: code_for_hw8/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/code_for_hw8/\n",
            "  inflating: __MACOSX/code_for_hw8/._.DS_Store  \n",
            "  inflating: code_for_hw8/q4.zip     \n",
            "  inflating: code_for_hw8/utils_hw8.py  \n",
            "  inflating: code_for_hw8/code_for_hw8_oop.py  \n",
            "  inflating: code_for_hw8/code_for_hw8_pytorch.py  \n",
            "   creating: code_for_hw8/data/\n",
            "  inflating: code_for_hw8/data/data3_train.csv  \n",
            "  inflating: code_for_hw8/data/data4_train.csv  \n",
            "  inflating: code_for_hw8/data/data4_validate.csv  \n",
            "  inflating: code_for_hw8/data/data3_validate.csv  \n",
            "  inflating: code_for_hw8/data/dataXor_train.csv  \n",
            "  inflating: code_for_hw8/data/data2_train.csv  \n",
            "  inflating: code_for_hw8/data/data2_validate.csv  \n",
            "  inflating: code_for_hw8/data/data3class_train.csv  \n",
            "  inflating: code_for_hw8/data/data1_validate.csv  \n",
            "  inflating: code_for_hw8/data/data1_train.csv  \n",
            "Archive:  code_for_hw8/q4.zip\n",
            "  inflating: test3.zip               \n",
            "  inflating: code_for_hw8_q4.py      \n",
            "  inflating: __MACOSX/._code_for_hw8_q4.py  \n",
            "  inflating: test2.zip               \n",
            "  inflating: test1.zip               \n",
            "  inflating: squeezenet_trained_cats_v_dogs.pt  \n",
            "  inflating: __MACOSX/._squeezenet_trained_cats_v_dogs.pt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-sSs7N4mMiX",
        "colab_type": "text"
      },
      "source": [
        "# 1) Implementing Mini-batch Gradient Descent and Batch Normalization\n",
        "\n",
        "** Note: You can click the arrow on the left of this text block to collapse/expand this optional section and all its code blocks **\n",
        "\n",
        "Last week we implemented a framework for building neural networks from scratch. We trained our models using *stochastic* gradient descent. In this problem, we explore how we can implement batch normalization as a module `BatchNorm` in our framework. It is the same module which you analyzed in problem 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgxmIfXVmVwd",
        "colab_type": "text"
      },
      "source": [
        "Key to the concept of batch normalization is the doing gradient descent on batches of data. So we instead of using last week's stochastic gradient descent, we will first implement the *mini-batch* gradient descent method `mini_gd`, which is a hybrid between *stochastic* gradient descent and *batch* gradient descent. The lecture notes on <a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week7/neural_networks_2/1?activate_block_id=block-v1%3AMITx%2B6.036%2B2019_Spring%2Btype%40vertical%2Bblock%40neural_networks_2_optimizing_neural_network_parameters_vert\"> optimizing neural network parameters</a> are helpful for this part.\n",
        "\n",
        "In *mini-batch* gradient descent, for a mini-batch of size $K$, we select $K$ distinct data points uniformly at random from the data set and update the network weights based only on their contributions to the gradient:\n",
        "$$W := W - \\eta\\sum_{i=1}^K \\nabla_W \\mathcal{L}(h(x^{(i)}; W), y^{(i)})\\;\\;.$$\n",
        "\n",
        "Our *mini-batch* method `mini_gd` will be implemented within the `Sequential` python class (see homework 7 problem 2) and will take the following as inputs:\n",
        "\n",
        "* `X`: a standard data array (d by n)\n",
        "* `y`: a standard labels row vector (1 by n)\n",
        "* `iters`: the number of updates to perform on weights $W$\n",
        "* `lrate`: the learning rate used\n",
        "* `K`: the mini-batch size to be used\n",
        "\n",
        "One call of `mini_gd` should call `Sequential.backward` for back-propagation and `Sequential.step` for updating the weights, for a total of `iters` times, using `lrate` as the learning rate. As in our implementation of `sgd` from homework 7, we compute the predicted output for a mini-batch of data with the `Sequential.forward` method. We compute the loss between our predictions and the true labels using the assigned `Sequential.loss` method. (Note that in homework 7, `Sequential.step` was called `Sequential.sgd_step`. While the functionality of the step function is the same, it has been renamed for convenience. The same is true for the `module.step` function of each module we implemented, where applicable.)\n",
        "\n",
        "For picking $K$ unique data points at random from our large data-set for each mini-batch, we will implement the following strategy: we will first shuffle our data points `X` (and associated labels `y`). Then, we get $\\frac{n}{k}$ (rounded down to the nearest integer) different mini-batches by grouping each $K$ consecutive points from this shuffled array. If we end up iterating over all the points but need more mini-batches, we will repeat the shuffling and the batching process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr1kWI08mdo4",
        "colab_type": "text"
      },
      "source": [
        "<b>1A)</b> You need to fill in the missing code below. We have implemented the shuffling of indices and have provided you with the outer and inner loops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_lvmO9Z22bH",
        "colab_type": "text"
      },
      "source": [
        "### PLEASE COPY IN YOUR CODE FROM HOMEWORK 7 TO COMPLEMENT THE CLASSES GIVEN HERE\n",
        "\n",
        "Recall that your implementation from homework 7 included the following classes:\n",
        "    \n",
        "  * Module\n",
        "  * Linear \n",
        "  * Tanh \n",
        "  * ReLU \n",
        "  * SoftMax\n",
        "  * NLL\n",
        "  * Sequential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y-Tigvq4gOEs",
        "colab": {}
      },
      "source": [
        "import math as m\n",
        "class Module:\n",
        "    def sgd_step(self, lrate): pass  # For modules w/o weights\n",
        "\n",
        "\n",
        "class Linear(Module):\n",
        "    def __init__(self, m, n):\n",
        "        self.m, self.n = (m, n)  # (in size, out size)\n",
        "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
        "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
        "\n",
        "    def forward(self, A):\n",
        "        # (m x b)  Hint: make sure you understand what b stands for\n",
        "        self.A = A\n",
        "        return self.W.T@A + self.W0  # (n x b)\n",
        "\n",
        "    # dLdZ is (n x b), uses stored self.A\n",
        "    def backward(self, dLdZ):\n",
        "        self.dLdW = self.A@dLdZ.T\n",
        "        self.dLdW0 = np.sum(dLdZ, axis=1, keepdims=True)\n",
        "        return self.W@dLdZ           # Return dLdA (m x b)\n",
        "\n",
        "    def sgd_step(self, lrate):  # Gradient descent step\n",
        "        self.W -= lrate * self.dLdW\n",
        "        self.W0 -= lrate * self.dLdW0\n",
        "\n",
        "class Tanh(Module):            # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = np.tanh(Z)\n",
        "        return self.A\n",
        "\n",
        "    # Uses stored self.A\n",
        "    def backward(self, dLdA):\n",
        "        # Your code: return dLdZ with dimensions (n, b) (?, b)\n",
        "        return dLdA * (1 - self.A**2)\n",
        "\n",
        "\n",
        "class ReLU(Module):              # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = np.maximum(Z, np.zeros(Z.shape))             # Your code: (n, b)\n",
        "        return self.A\n",
        "\n",
        "    # uses stored self.A\n",
        "    def backward(self, dLdA):\n",
        "        # Your code: return dLdZ (m, b)\n",
        "        return dLdA * (self.A != 0)\n",
        "\n",
        "\n",
        "class SoftMax(Module):           # Output activation\n",
        "    def forward(self, Z):\n",
        "        # Your code: (n, b)\n",
        "        return np.apply_along_axis(lambda col: np.exp(col) / np.sum(np.exp(col)), axis=0, arr=Z)              # Your code: (n, b)\n",
        "\n",
        "    # Assume that dLdZ is passed in\n",
        "    def backward(self, dLdZ):\n",
        "        return dLdZ\n",
        "\n",
        "    def class_fun(self, Ypred):  # Return class indices\n",
        "        return np.argmax(Ypred, axis=0)              # Your code: A 1D vector (b, ) \n",
        "\n",
        "\n",
        "class NLL(Module):       # Loss\n",
        "    def forward(self, Ypred, Y):\n",
        "        self.Ypred = Ypred\n",
        "        self.Y = Y\n",
        "        return -np.sum(Y * np.log(Ypred))      # Your code\n",
        "\n",
        "    def backward(self):  # Use stored self.Ypred, self.Y\n",
        "        return self.Ypred - self.Y      # Your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHso5_RFrgnF",
        "colab_type": "text"
      },
      "source": [
        "Implement `mini_gd` in `Sequential` below.\n",
        "\n",
        "**Hint:** The documentation for <a href=\"https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.shuffle.html\"> `numpy.random.shuffle`</a> might be helpful for this part. If you have a list of elements `l` and a set of indices `indices`, you can call `numpy.random.shuffle(indices)` and `l = l[indices]` to shuffle the elements of `l`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brq8yO4trfCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math as m\n",
        "class Sequential:\n",
        "    def __init__(self, modules, loss):            \n",
        "        self.modules = modules\n",
        "        self.loss = loss\n",
        "\n",
        "    def mini_gd(self, X, Y, iters, lrate, notif_each=None, K=10):\n",
        "        D, N = X.shape\n",
        "\n",
        "        np.random.seed(0)\n",
        "        num_updates = 0\n",
        "        indices = np.arange(N)\n",
        "        while num_updates < iters:\n",
        "\n",
        "            np.random.shuffle(indices)\n",
        "            X = X[:, indices]\n",
        "            Y = Y[:, indices]\n",
        "\n",
        "            for j in range(m.floor(N/K)):\n",
        "                if num_updates >= iters: break\n",
        "\n",
        "                # Implement the main part of mini_gd here\n",
        "                Xt = X[:, j*K:(j+1)*K]\n",
        "                Yt = Y[:, j*K:(j+1)*K]\n",
        "\n",
        "                # The rest of this function should be similar to your\n",
        "                # implementation of Sequential.sgd in HW 7\n",
        "                Ypred = self.forward(Xt)\n",
        "\n",
        "                loss = self.loss.forward(Ypred, Yt)\n",
        "\n",
        "                delta = self.loss.backward()\n",
        "                self.backward(delta)\n",
        "                self.sgd_step(lrate)\n",
        "                \n",
        "                num_updates += 1\n",
        "\n",
        "    def forward(self, Xt):                        \n",
        "        for m in self.modules: Xt = m.forward(Xt)\n",
        "        return Xt\n",
        "\n",
        "    def backward(self, delta):                   \n",
        "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
        "\n",
        "    def sgd_step(self, lrate):    \n",
        "        for m in self.modules: m.sgd_step(lrate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JZeeKXkm6YI",
        "colab_type": "text"
      },
      "source": [
        "<b>1B)</b> We are now ready to implement batch normalization into our neural network framework! Our module `BatchNorm` will sit between consecutive layers of neurons, such as the $l^{th}$ and $(l+1)^{th}$ layers, acting as a \"corrector\" which allows $W^l$ to change freely, producing outputs $z^l$, but then the module corrects the covariate shift induced in the signals before they reach the $(l+1)^{th}$ layer, converting $z^l$ to $\\widehat{Z}^l$. \n",
        "\n",
        "The following is a summmary what is described in the <a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week7/neural_networks_2/2\">lecture notes</a>, and it should guide your implementation of the module. \n",
        "\n",
        "Any normalization between the $l^{th}$ and $(l+1)^{th}$ layers is done *separately* for each of the $n^l$ input connections leading to the $(l+1)^{th}$ layer. We handle a mini-batch of data of size $K$, and $Z^l$ is $n^l \\times K$, and the output $\\widehat{Z}^l$is of the same shape. \n",
        "\n",
        "We first compute $n^l$ *batchwise* means and\n",
        "standard deviations.  Let $\\mu^l$ be the $n^l \\times 1$ vector (`self.mus`) where\n",
        "$$\\mu^l_i = \\frac{1}{K} \\sum_{j = 1}^K Z^l_{ij}\\;\\;,$$\n",
        "and let $\\sigma^l$ be the $n^l \\times 1$ vector (`self.vars`) where \n",
        "$$\\sigma^l_i = \\sqrt{\\frac{1}{K} \\sum_{j = 1}^K (Z^l_{ij} - \\mu_i)^2}\\;\\;.$$\n",
        "\n",
        "The normalized data `self.norm` is the matrix $\\overline{Z}$, where\n",
        "$$\\overline{Z}^l_{ij} = \\frac{Z^l_{ij} - \\mu^l_i}{\\sigma^l_i + \\epsilon}\\;\\;,$$\n",
        "and where $\\epsilon$ is a very small constant to guard against division by\n",
        "zero. \n",
        "\n",
        "We define weights $G^l$ (`self.G`) and $B^l$ (`self.B`), each being an $n^l \\times 1$ vector, which we use to to shift and scale the outputs:\n",
        "$$\\widehat{Z}^l_{ij} = G^l_i \\overline{Z}^l_{ij} + B^l_i\\;\\;.$$\n",
        "\n",
        "The outputs are finally ready to be passed to the $(l+1)^{th}$ layer.\n",
        "\n",
        "A slight warning (that we will not worry about here) about `BatchNorm` is that during the *test* phase, if the test mini-batch size is too small (imagine we are deploying a neural network that deals with live video frames), then the lack of samples would cause the freshly-calculated $\\mu^l$ and $\\sigma^l$ to be far off from their true values that the module's parameters $G^l$ and $B^l$ were trained to be compatible with. To fix that, people usually compute a running average of $\\mu^l$ and $\\sigma^l$ during training, to be used at test time. We will assume our test mini-batches are large enough.\n",
        "\n",
        "In this problem we only implement the `BatchNorm.forward` and `BatchNorm.step` methods. We provide you with the implementation for `BatchNorm.backward` and the lecture notes contain the details of the derivations. You will need to fill in the missing code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlXP26plm8R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchNorm(Module):    \n",
        "    def __init__(self, m):\n",
        "        np.random.seed(0)\n",
        "        self.eps = 1e-20\n",
        "        self.m = m  # number of input channels\n",
        "        \n",
        "        # Init learned shifts and scaling factors\n",
        "        self.B = np.zeros([self.m, 1]) # m x 1\n",
        "        self.G = np.random.normal(0, 1.0 * self.m ** (-.5), [self.m, 1]) # m x 1\n",
        "        \n",
        "    # Works on m x b matrices of m input channels and b different inputs\n",
        "    def forward(self, A):# A is m x K: m input channels and mini-batch size K\n",
        "        # Store last inputs and K for next backward() call\n",
        "        self.A = A\n",
        "        self.K = A.shape[1]\n",
        "        \n",
        "        self.mus = np.mean(A, axis=1, keepdims=True)\n",
        "        self.vars = np.var(A, axis=1, keepdims=True)\n",
        "\n",
        "        # Normalize inputs using their mean and standard deviation\n",
        "        self.norm = (A - self.mus) / (np.sqrt(self.vars) + self.eps)\n",
        "            \n",
        "        # Return scaled and shifted versions of self.norm\n",
        "        return self.G * self.norm + self.B\n",
        "\n",
        "    def backward(self, dLdZ):\n",
        "        # Re-usable constants\n",
        "        std_inv = 1/np.sqrt(self.vars+self.eps)\n",
        "        A_min_mu = self.A-self.mus\n",
        "        \n",
        "        dLdnorm = dLdZ * self.G\n",
        "        dLdVar = np.sum(dLdnorm * A_min_mu * -0.5 * std_inv**3, axis=1, keepdims=True)\n",
        "        dLdMu = np.sum(dLdnorm*(-std_inv), axis=1, keepdims=True) + dLdVar * (-2/self.K) * np.sum(A_min_mu, axis=1, keepdims=True)\n",
        "        dLdX = (dLdnorm * std_inv) + (dLdVar * (2/self.K) * A_min_mu) + (dLdMu/self.K)\n",
        "        \n",
        "        self.dLdB = np.sum(dLdZ, axis=1, keepdims=True)\n",
        "        self.dLdG = np.sum(dLdZ * self.norm, axis=1, keepdims=True)\n",
        "        return dLdX\n",
        "\n",
        "    def sgd_step(self, lrate):\n",
        "        self.B = self.B - lrate*self.dLdB\n",
        "        self.G = self.G - lrate*self.dLdG"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4u39OCjLza",
        "colab_type": "text"
      },
      "source": [
        "# 2) Weight sharing (OPTIONAL)\n",
        "\n",
        "** Note: You can click the arrow on the left of this text block to collapse/expand this optional section and all its code blocks **\n",
        "\n",
        "In the lab we designed a CNN that can count the number of objects in 1 dimensional images, where each black pixel is represented by a value of 0 and each white pixel is represented by a value of 1. Recall that an object is a consecutive sequence of black pixels ($0$'s). For example, the sequence $0100110$ contains three objects. \n",
        "\n",
        "In this problem we want to see how hard/easy it is to train such a network from data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp8stejLA57F",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Our network architecture will be as follows:\n",
        "\n",
        "*    The first layer is convolutional and you will implement it using the PyTorch `torch.nn.Conv1d` function, with a kernel of size 2 and stride of 1, followed by a ReLu activation (`torch.nn.ReLU`).\n",
        "\n",
        "*    The second layer is a fully connected `torch.nn.Linear` layer which has a scalar output.\n",
        "\n",
        "Here is sample usage of the `Conv1d` and `Linear` layers. \n",
        "\n",
        "`layer1=torch.nn.Conv1d(in_channels=?, out_channels=?, kernel_size=?,stride=?,padding=?,bias=True)`\n",
        "\n",
        "Here, `in_channels` is the number of channels in your data (so for example, RGB images have 3 channels). You can think of the `out_channels` variable as the number of filters you are using.\n",
        "\n",
        "`layer3 = torch.nn.Linear(in_units=?, out_units=?)`\n",
        "\n",
        "You need to fill in the parameters marked with `?` based on the problem specifications. Note also that in PyTorch, depending on your implementation, you may be forced to use *three* (four if we count ReLU) layers to implement such a network, where one intermediary `Flatten` layer is used to flatten the output of the convolutional layer, before being passed to the dense layer.\n",
        "\n",
        "Refer to the <a href=\"https://pytorch.org/docs/stable/nn.html#conv1d\">Conv1D</a>, <a href=\"https://pytorch.org/docs/stable/nn.html#linear\">Linear</a> and <a href=\"https://pytorch.org/docs/stable/nn.html#flatten\">Flatten</a> descriptions in the PyTorch documentation to see the available parameter options.\n",
        "\n",
        "In this exercise, we fix the structure and want to learn the best combination of weights from data. In the homework code, we have provided functions `train_neural_counter` and `get_image_data_1d`. You can use them to generate data and train the above neural network in PyTorch to answer the following questions. We assume that the images in our data set are randomly generated. The probability of a pixel being white is $0.1$. We work with mean squared error as the loss function for this problem. We have provided template code which you can fill in, to perform the training.\n",
        "\n",
        "We have also provided helper functions such as `set_weight`, `set_bias`, which might help you set weights and biases of a particular layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKa8iMv_j3ek",
        "colab_type": "text"
      },
      "source": [
        "<b>2B)</b> What is (approximately) the expected loss of the network on $1024\\times 1$ images if the convolutional layer is an averaging filter and second layer is the sum function (without a bias term)? (Note that you can answer the question theoretically or through coding, depending on your preference.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPcB588ok8a",
        "colab_type": "code",
        "outputId": "2c92d540-7288-4103-e880-d43d0d05ebe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Code template if you would like to check 2B) through code\n",
        "tsize = 1000\n",
        "imsize = 1024\n",
        "prob_white = 0.1\n",
        "\n",
        "(X_train,Y_train,X_val,Y_val,X_test,Y_test) = get_image_data_1d(tsize,imsize,prob_white)\n",
        "test_loader = make_iter(X_test, Y_test)\n",
        "\n",
        "num_filters = 1\n",
        "kernel_size = 2\n",
        "strides = 1\n",
        "padding = 1\n",
        "\n",
        "layer_1 = nn.Conv1d(\n",
        "    in_channels=1,\n",
        "    out_channels=num_filters,\n",
        "    kernel_size=kernel_size,\n",
        "    stride=strides,\n",
        "    padding=padding,\n",
        "    bias=False\n",
        ")\n",
        "\n",
        "num_units = imsize+1\n",
        "layer_3 = nn.Linear(num_units, 1, bias=False)\n",
        "layers = [layer_1, nn.ReLU(), nn.Flatten(), layer_3]\n",
        "model = nn.Sequential(*layers)\n",
        "\n",
        "set_weights(model[0], np.array([1/2,1/2]))\n",
        "set_weights(model[-1], np.ones(num_units))\n",
        "\n",
        "model_evaluate(model, test_loader, MSELoss())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100.567, 100.567)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js3OYsbwj7Ms",
        "colab_type": "text"
      },
      "source": [
        "<b>2C)</b> Now suppose we add a bias term of $-10$ to the last layer. What is (approximately) the expected quadratic loss? (Note that you can answer the question theoretically or through coding, depending on your preference.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKynxhF1klga",
        "colab_type": "code",
        "outputId": "16d679fe-4ac6-4cac-f8d5-0dfe81f0dd40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Edit code from 2B) with the bias\n",
        "bias = -10\n",
        "set_bias(model[-1], bias)\n",
        "model_evaluate(model, test_loader, MSELoss())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12.507, 12.507)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GCLr8qmj-Hk",
        "colab_type": "text"
      },
      "source": [
        "<b>2D)</b> Averaging type filters are abundant and form a nearly flat valley of local minima for this problem. It is difficult for the network to find alternative solutions on its own. We need to force our way out of these bad minima and towards a better solution, i.e., an edge detector. To force the first layer to behave as an edge detector, we need to choose a proper **kernel regularizer**. Consider the following functions\n",
        "\n",
        "$f_1=\\sum_i |w_i|$, $f_2=\\sum_i |w_i^2|$, $f_3=|\\sum_{i} w_i|$. Which one of the choices is likely to guide the network to find an edge detector at the convolution layer?\n",
        "\n",
        "\n",
        "<a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2020_Spring/courseware/Week8/week8_homework/\">Refer to HW8 on MITx.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aubU6Q6kwOI",
        "colab_type": "text"
      },
      "source": [
        "Implement your choice of regularizers from above in the code (complete the function `filter_reg`). Do not allow any bias in the layers for the rest of the problem. The code generates some random test and training data sets and trains the model on these data. Run a few learning trials (5 or more) for each data set and answer the following questions based on the performance of your model.\n",
        "\n",
        "**IMPORTANT**: When implementing `filter_reg`, you should use the torch backend operations, imported as \"torch\" in the code. So for example, `torch.sum` and `torch.abs`, rather than `np.sum` and `np.abs`. This is because the `weights` argument is NOT a numpy object, but rather an internal torch object!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOLZf_JsuTLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement filter_reg\n",
        "def filter_reg(weights, lam=1000):\n",
        "    # We scale the output of the filter by lam\n",
        "    filter_result = torch.abs(torch.sum(weights))\n",
        "    return lam * filter_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bXvcRuk_3q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_reg(model, lam=1000):\n",
        "    # Don't edit this function!\n",
        "    filter_weights = model[0].weight\n",
        "    return filter_reg(filter_weights, lam)\n",
        "\n",
        "def train_neural_counter(layers, data, regularize=False, lam=1000, display=False):\n",
        "    (X_train, Y_train, X_val, Y_val, X_test, Y_test) = data\n",
        "    epochs = 10\n",
        "    batch = 1\n",
        "\n",
        "    train_iter, val_iter, test_iter = (make_iter(X_train, Y_train),\n",
        "                                       make_iter(X_val,Y_val),\n",
        "                                       make_iter(X_test,Y_test))\n",
        "    model = nn.Sequential(*layers)\n",
        "    optimizer = Adam(model.parameters())\n",
        "    criterion = MSELoss()\n",
        "\n",
        "    if regularize:\n",
        "        regularizer = lambda model: model_reg(model, lam)\n",
        "    else:\n",
        "        regularizer = None\n",
        "\n",
        "    model_fit(model, train_iter, epochs, optimizer, criterion, val_iter, \n",
        "              history=None,verbose=True, model_reg=regularizer)\n",
        "    err = model_evaluate(model, test_iter, criterion)\n",
        "    ws = model[-1].weight\n",
        "    if display:\n",
        "        plt.plot(ws)\n",
        "        plt.show()\n",
        "    return model,err"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYRwd0eJkAdh",
        "colab_type": "text"
      },
      "source": [
        "<b>2E)</b> For $1024\\times 1$ images and training set of size $1000$, is the network **without any regularization** likely to find models that have a mean square error lower than 8 on the test data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KiCbZmksXO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imsize = 1024\n",
        "prob_white = 0.1\n",
        "\n",
        "data=get_image_data_1d(1000, imsize, prob_white)\n",
        "\n",
        "def run_trials(trials=5, lam=None):\n",
        "    for trial in range(trials):\n",
        "        num_filters = 1\n",
        "        kernel_size = 2\n",
        "        strides = 1\n",
        "        padding = 1\n",
        "\n",
        "        layer_1 = nn.Conv1d(\n",
        "            in_channels=1,\n",
        "            out_channels=num_filters,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=strides,\n",
        "            padding=padding,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        num_units = imsize + 1\n",
        "        layer_3 = nn.Linear(num_units, 1, bias=False)\n",
        "        layers = [layer_1, nn.ReLU(), nn.Flatten(), layer_3]\n",
        "        model, err=train_neural_counter(layers, data, regularize=lam is not None, lam=lam)\n",
        "        print(model[0].weight)\n",
        "        print(torch.mean(model[-1].weight))\n",
        "        print(err)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm0Jqb8QPvbk",
        "colab_type": "code",
        "outputId": "6d0a7667-6572-4448-a2cb-18c8998efa88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "run_trials()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 1 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 2 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 3 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 4 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 5 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 6 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 7 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 8 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 9 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "Parameter containing:\n",
            "tensor([[[-0.6691, -0.1608]]], requires_grad=True)\n",
            "tensor(1.2457e-07, grad_fn=<MeanBackward0>)\n",
            "(8715.09, 8715.09)\n",
            "epoch: 0 | TRAIN: loss 3693.79556 acc 3693.79556 | VALI: loss 31.04186 acc 31.04186\n",
            "epoch: 1 | TRAIN: loss 11.66251 acc 11.66251 | VALI: loss 10.75434 acc 10.75434\n",
            "epoch: 2 | TRAIN: loss 9.62954 acc 9.62954 | VALI: loss 10.82398 acc 10.82398\n",
            "epoch: 3 | TRAIN: loss 8.90232 acc 8.90232 | VALI: loss 10.93106 acc 10.93106\n",
            "epoch: 4 | TRAIN: loss 8.04706 acc 8.04706 | VALI: loss 11.16735 acc 11.16735\n",
            "epoch: 5 | TRAIN: loss 7.20181 acc 7.20181 | VALI: loss 11.65159 acc 11.65159\n",
            "epoch: 6 | TRAIN: loss 6.46295 acc 6.46295 | VALI: loss 12.36852 acc 12.36852\n",
            "epoch: 7 | TRAIN: loss 5.8545 acc 5.8545 | VALI: loss 13.23571 acc 13.23571\n",
            "epoch: 8 | TRAIN: loss 5.36296 acc 5.36296 | VALI: loss 14.17931 acc 14.17931\n",
            "epoch: 9 | TRAIN: loss 4.96526 acc 4.96526 | VALI: loss 15.15054 acc 15.15054\n",
            "Parameter containing:\n",
            "tensor([[[1.6259, 1.0266]]], requires_grad=True)\n",
            "tensor(0.3404, grad_fn=<MeanBackward0>)\n",
            "(13.42342075217783, 13.42342075217783)\n",
            "epoch: 0 | TRAIN: loss 3761.75834 acc 3761.75834 | VALI: loss 26.96146 acc 26.96146\n",
            "epoch: 1 | TRAIN: loss 11.23917 acc 11.23917 | VALI: loss 10.67239 acc 10.67239\n",
            "epoch: 2 | TRAIN: loss 9.56258 acc 9.56258 | VALI: loss 10.76086 acc 10.76086\n",
            "epoch: 3 | TRAIN: loss 8.85582 acc 8.85582 | VALI: loss 10.88634 acc 10.88634\n",
            "epoch: 4 | TRAIN: loss 8.03115 acc 8.03115 | VALI: loss 11.13381 acc 11.13381\n",
            "epoch: 5 | TRAIN: loss 7.2227 acc 7.2227 | VALI: loss 11.59911 acc 11.59911\n",
            "epoch: 6 | TRAIN: loss 6.52004 acc 6.52004 | VALI: loss 12.261 acc 12.261\n",
            "epoch: 7 | TRAIN: loss 5.94357 acc 5.94357 | VALI: loss 13.04944 acc 13.04944\n",
            "epoch: 8 | TRAIN: loss 5.47846 acc 5.47846 | VALI: loss 13.90369 acc 13.90369\n",
            "epoch: 9 | TRAIN: loss 5.10149 acc 5.10149 | VALI: loss 14.78328 acc 14.78328\n",
            "Parameter containing:\n",
            "tensor([[[1.4024, 1.0162]]], requires_grad=True)\n",
            "tensor(0.3732, grad_fn=<MeanBackward0>)\n",
            "(13.154305967966618, 13.154305967966618)\n",
            "epoch: 0 | TRAIN: loss 3019.08364 acc 3019.08364 | VALI: loss 16.84691 acc 16.84691\n",
            "epoch: 1 | TRAIN: loss 10.77528 acc 10.77528 | VALI: loss 11.07528 acc 11.07528\n",
            "epoch: 2 | TRAIN: loss 9.6803 acc 9.6803 | VALI: loss 11.08946 acc 11.08946\n",
            "epoch: 3 | TRAIN: loss 8.8655 acc 8.8655 | VALI: loss 11.19843 acc 11.19843\n",
            "epoch: 4 | TRAIN: loss 7.95412 acc 7.95412 | VALI: loss 11.51406 acc 11.51406\n",
            "epoch: 5 | TRAIN: loss 7.10255 acc 7.10255 | VALI: loss 12.12182 acc 12.12182\n",
            "epoch: 6 | TRAIN: loss 6.39226 acc 6.39226 | VALI: loss 12.97197 acc 12.97197\n",
            "epoch: 7 | TRAIN: loss 5.82192 acc 5.82192 | VALI: loss 13.96077 acc 13.96077\n",
            "epoch: 8 | TRAIN: loss 5.36466 acc 5.36466 | VALI: loss 15.00507 acc 15.00507\n",
            "epoch: 9 | TRAIN: loss 4.99488 acc 4.99488 | VALI: loss 16.05471 acc 16.05471\n",
            "Parameter containing:\n",
            "tensor([[[1.2398, 1.6853]]], requires_grad=True)\n",
            "tensor(0.3089, grad_fn=<MeanBackward0>)\n",
            "(13.75673833384947, 13.75673833384947)\n",
            "epoch: 0 | TRAIN: loss 3684.31511 acc 3684.31511 | VALI: loss 31.74109 acc 31.74109\n",
            "epoch: 1 | TRAIN: loss 11.82283 acc 11.82283 | VALI: loss 10.87628 acc 10.87628\n",
            "epoch: 2 | TRAIN: loss 9.674 acc 9.674 | VALI: loss 10.93502 acc 10.93502\n",
            "epoch: 3 | TRAIN: loss 8.91896 acc 8.91896 | VALI: loss 11.05626 acc 11.05626\n",
            "epoch: 4 | TRAIN: loss 8.0335 acc 8.0335 | VALI: loss 11.32212 acc 11.32212\n",
            "epoch: 5 | TRAIN: loss 7.16473 acc 7.16473 | VALI: loss 11.84401 acc 11.84401\n",
            "epoch: 6 | TRAIN: loss 6.41366 acc 6.41366 | VALI: loss 12.60668 acc 12.60668\n",
            "epoch: 7 | TRAIN: loss 5.80097 acc 5.80097 | VALI: loss 13.52295 acc 13.52295\n",
            "epoch: 8 | TRAIN: loss 5.30858 acc 5.30858 | VALI: loss 14.51259 acc 14.51259\n",
            "epoch: 9 | TRAIN: loss 4.91106 acc 4.91106 | VALI: loss 15.52297 acc 15.52297\n",
            "Parameter containing:\n",
            "tensor([[[1.6911, 1.0236]]], requires_grad=True)\n",
            "tensor(0.3327, grad_fn=<MeanBackward0>)\n",
            "(13.655064831581258, 13.655064831581258)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1vcUEL-vW9D",
        "colab_type": "text"
      },
      "source": [
        "#### For parts F) to J), simply edit your code from E) with the necessary changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_25ygQJkD5F",
        "colab_type": "text"
      },
      "source": [
        "<b>2F)</b> Repeat the same experiment, but now with the regularizer you implemented. Try different regularization parameters. Which choice of regularization parameter gives the best prediction results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNAChIqylIlt",
        "colab_type": "code",
        "outputId": "de186046-f439-4c4d-e789-3b9b139d8887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for lam in [0, 1, 10, 1000]:\n",
        "    run_trials(trials=3, lam=lam)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 1 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 2 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 3 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 4 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 5 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 6 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 7 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 8 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "epoch: 9 | TRAIN: loss 8679.942 acc 8679.942 | VALI: loss 8777.495 acc 8777.495\n",
            "Parameter containing:\n",
            "tensor([[[-0.0079, -0.1442]]], requires_grad=True)\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>)\n",
            "(8715.09, 8715.09)\n",
            "epoch: 0 | TRAIN: loss 4136.56289 acc 4136.56289 | VALI: loss 79.9457 acc 79.9457\n",
            "epoch: 1 | TRAIN: loss 15.94536 acc 15.94536 | VALI: loss 10.75934 acc 10.75934\n",
            "epoch: 2 | TRAIN: loss 9.82329 acc 9.82329 | VALI: loss 10.82952 acc 10.82952\n",
            "epoch: 3 | TRAIN: loss 9.05411 acc 9.05411 | VALI: loss 10.95698 acc 10.95698\n",
            "epoch: 4 | TRAIN: loss 8.12666 acc 8.12666 | VALI: loss 11.19886 acc 11.19886\n",
            "epoch: 5 | TRAIN: loss 7.18927 acc 7.18927 | VALI: loss 11.68842 acc 11.68842\n",
            "epoch: 6 | TRAIN: loss 6.36379 acc 6.36379 | VALI: loss 12.44882 acc 12.44882\n",
            "epoch: 7 | TRAIN: loss 5.68632 acc 5.68632 | VALI: loss 13.40026 acc 13.40026\n",
            "epoch: 8 | TRAIN: loss 5.14227 acc 5.14227 | VALI: loss 14.45321 acc 14.45321\n",
            "epoch: 9 | TRAIN: loss 4.70524 acc 4.70524 | VALI: loss 15.54479 acc 15.54479\n",
            "Parameter containing:\n",
            "tensor([[[1.9595, 0.8609]]], requires_grad=True)\n",
            "tensor(0.3200, grad_fn=<MeanBackward0>)\n",
            "(13.669170114050212, 13.669170114050212)\n",
            "epoch: 0 | TRAIN: loss 2820.78501 acc 2820.78501 | VALI: loss 14.67885 acc 14.67885\n",
            "epoch: 1 | TRAIN: loss 10.37642 acc 10.37642 | VALI: loss 11.07597 acc 11.07597\n",
            "epoch: 2 | TRAIN: loss 9.49722 acc 9.49722 | VALI: loss 11.10064 acc 11.10064\n",
            "epoch: 3 | TRAIN: loss 8.71591 acc 8.71591 | VALI: loss 11.19886 acc 11.19886\n",
            "epoch: 4 | TRAIN: loss 7.84836 acc 7.84836 | VALI: loss 11.50341 acc 11.50341\n",
            "epoch: 5 | TRAIN: loss 7.04336 acc 7.04336 | VALI: loss 12.11484 acc 12.11484\n",
            "epoch: 6 | TRAIN: loss 6.37268 acc 6.37268 | VALI: loss 12.98042 acc 12.98042\n",
            "epoch: 7 | TRAIN: loss 5.83316 acc 5.83316 | VALI: loss 13.98899 acc 13.98899\n",
            "epoch: 8 | TRAIN: loss 5.40079 acc 5.40079 | VALI: loss 15.05215 acc 15.05215\n",
            "epoch: 9 | TRAIN: loss 5.05209 acc 5.05209 | VALI: loss 16.11757 acc 16.11757\n",
            "Parameter containing:\n",
            "tensor([[[1.4512, 1.5445]]], requires_grad=True)\n",
            "tensor(0.3016, grad_fn=<MeanBackward0>)\n",
            "(13.9372617421326, 13.9372617421326)\n",
            "epoch: 0 | TRAIN: loss 3737.6492 acc 3735.88986 | VALI: loss 37.13933 acc 37.13933\n",
            "epoch: 1 | TRAIN: loss 14.85588 acc 12.10186 | VALI: loss 10.80569 acc 10.80569\n",
            "epoch: 2 | TRAIN: loss 12.29691 acc 9.53649 | VALI: loss 10.91098 acc 10.91098\n",
            "epoch: 3 | TRAIN: loss 11.55893 acc 8.80027 | VALI: loss 11.06599 acc 11.06599\n",
            "epoch: 4 | TRAIN: loss 10.69474 acc 7.93816 | VALI: loss 11.36002 acc 11.36002\n",
            "epoch: 5 | TRAIN: loss 9.84482 acc 7.09068 | VALI: loss 11.91202 acc 11.91202\n",
            "epoch: 6 | TRAIN: loss 9.10336 acc 6.35217 | VALI: loss 12.69862 acc 12.69862\n",
            "epoch: 7 | TRAIN: loss 8.49006 acc 5.7424 | VALI: loss 13.62602 acc 13.62602\n",
            "epoch: 8 | TRAIN: loss 7.9905 acc 5.247 | VALI: loss 14.61439 acc 14.61439\n",
            "epoch: 9 | TRAIN: loss 7.5829 acc 4.84414 | VALI: loss 15.61425 acc 15.61425\n",
            "Parameter containing:\n",
            "tensor([[[0.9774, 1.7559]]], requires_grad=True)\n",
            "tensor(0.3303, grad_fn=<MeanBackward0>)\n",
            "(13.507354249181518, 13.507354249181518)\n",
            "epoch: 0 | TRAIN: loss 4230.42689 acc 4229.31428 | VALI: loss 33.29974 acc 33.29974\n",
            "epoch: 1 | TRAIN: loss 13.67915 acc 11.60327 | VALI: loss 10.79242 acc 10.79242\n",
            "epoch: 2 | TRAIN: loss 11.66712 acc 9.58643 | VALI: loss 10.86321 acc 10.86321\n",
            "epoch: 3 | TRAIN: loss 10.99345 acc 8.91379 | VALI: loss 10.95704 acc 10.95704\n",
            "epoch: 4 | TRAIN: loss 10.19364 acc 8.11506 | VALI: loss 11.14563 acc 11.14563\n",
            "epoch: 5 | TRAIN: loss 9.39541 acc 7.31789 | VALI: loss 11.51137 acc 11.51137\n",
            "epoch: 6 | TRAIN: loss 8.69305 acc 6.61653 | VALI: loss 12.04018 acc 12.04018\n",
            "epoch: 7 | TRAIN: loss 8.11513 acc 6.03959 | VALI: loss 12.67939 acc 12.67939\n",
            "epoch: 8 | TRAIN: loss 7.64954 acc 5.57504 | VALI: loss 13.38166 acc 13.38166\n",
            "epoch: 9 | TRAIN: loss 7.27229 acc 5.19898 | VALI: loss 14.11374 acc 14.11374\n",
            "Parameter containing:\n",
            "tensor([[[0.8750, 1.1943]]], requires_grad=True)\n",
            "tensor(0.4361, grad_fn=<MeanBackward0>)\n",
            "(12.660906085474009, 12.660906085474009)\n",
            "epoch: 0 | TRAIN: loss 3223.19628 acc 3221.27539 | VALI: loss 19.58913 acc 19.58913\n",
            "epoch: 1 | TRAIN: loss 13.48806 acc 10.75103 | VALI: loss 10.83289 acc 10.83289\n",
            "epoch: 2 | TRAIN: loss 12.2325 acc 9.49304 | VALI: loss 10.92069 acc 10.92069\n",
            "epoch: 3 | TRAIN: loss 11.48062 acc 8.74302 | VALI: loss 11.06394 acc 11.06394\n",
            "epoch: 4 | TRAIN: loss 10.62249 acc 7.88704 | VALI: loss 11.36205 acc 11.36205\n",
            "epoch: 5 | TRAIN: loss 9.80147 acc 7.06846 | VALI: loss 11.91576 acc 11.91576\n",
            "epoch: 6 | TRAIN: loss 9.10213 acc 6.37197 | VALI: loss 12.68981 acc 12.68981\n",
            "epoch: 7 | TRAIN: loss 8.5339 acc 5.8071 | VALI: loss 13.59277 acc 13.59277\n",
            "epoch: 8 | TRAIN: loss 8.07609 acc 5.35319 | VALI: loss 14.54916 acc 14.54916\n",
            "epoch: 9 | TRAIN: loss 7.70443 acc 4.98593 | VALI: loss 15.51258 acc 15.51258\n",
            "Parameter containing:\n",
            "tensor([[[1.1337, 1.5797]]], requires_grad=True)\n",
            "tensor(0.3328, grad_fn=<MeanBackward0>)\n",
            "(13.594763298616744, 13.594763298616744)\n",
            "epoch: 0 | TRAIN: loss 3027.9751 acc 3007.982 | VALI: loss 16.23613 acc 16.23613\n",
            "epoch: 1 | TRAIN: loss 38.01418 acc 10.5715 | VALI: loss 10.9522 acc 10.9522\n",
            "epoch: 2 | TRAIN: loss 36.8742 acc 9.56403 | VALI: loss 11.04351 acc 11.04351\n",
            "epoch: 3 | TRAIN: loss 35.85875 acc 8.79759 | VALI: loss 11.18223 acc 11.18223\n",
            "epoch: 4 | TRAIN: loss 34.66188 acc 7.94942 | VALI: loss 11.45554 acc 11.45554\n",
            "epoch: 5 | TRAIN: loss 33.43138 acc 7.16137 | VALI: loss 11.93332 acc 11.93332\n",
            "epoch: 6 | TRAIN: loss 32.2613 acc 6.50438 | VALI: loss 12.56428 acc 12.56428\n",
            "epoch: 7 | TRAIN: loss 31.17991 acc 5.97902 | VALI: loss 13.26143 acc 13.26143\n",
            "epoch: 8 | TRAIN: loss 30.18583 acc 5.56179 | VALI: loss 13.96119 acc 13.96119\n",
            "epoch: 9 | TRAIN: loss 29.26898 acc 5.22797 | VALI: loss 14.62931 acc 14.62931\n",
            "Parameter containing:\n",
            "tensor([[[1.1762, 1.1956]]], requires_grad=True)\n",
            "tensor(0.3804, grad_fn=<MeanBackward0>)\n",
            "(12.922248053985998, 12.922248053985998)\n",
            "epoch: 0 | TRAIN: loss 4483.7079 acc 4472.12593 | VALI: loss 70.24034 acc 70.24034\n",
            "epoch: 1 | TRAIN: loss 37.56522 acc 14.5742 | VALI: loss 10.86733 acc 10.86733\n",
            "epoch: 2 | TRAIN: loss 32.66459 acc 9.66598 | VALI: loss 10.95671 acc 10.95671\n",
            "epoch: 3 | TRAIN: loss 31.82132 acc 8.98616 | VALI: loss 11.06762 acc 11.06762\n",
            "epoch: 4 | TRAIN: loss 30.77444 acc 8.17313 | VALI: loss 11.24521 acc 11.24521\n",
            "epoch: 5 | TRAIN: loss 29.65104 acc 7.35317 | VALI: loss 11.57942 acc 11.57942\n",
            "epoch: 6 | TRAIN: loss 28.56527 acc 6.62574 | VALI: loss 12.06561 acc 12.06561\n",
            "epoch: 7 | TRAIN: loss 27.56955 acc 6.02361 | VALI: loss 12.64575 acc 12.64575\n",
            "epoch: 8 | TRAIN: loss 26.66993 acc 5.5369 | VALI: loss 13.26542 acc 13.26542\n",
            "epoch: 9 | TRAIN: loss 25.85368 acc 5.14237 | VALI: loss 13.88894 acc 13.88894\n",
            "Parameter containing:\n",
            "tensor([[[1.3203, 0.7256]]], requires_grad=True)\n",
            "tensor(0.4405, grad_fn=<MeanBackward0>)\n",
            "(12.477954211364617, 12.477954211364617)\n",
            "epoch: 0 | TRAIN: loss 4145.1422 acc 4133.24506 | VALI: loss 35.9025 acc 35.9025\n",
            "epoch: 1 | TRAIN: loss 33.57694 acc 11.76317 | VALI: loss 10.94738 acc 10.94738\n",
            "epoch: 2 | TRAIN: loss 31.30742 acc 9.53113 | VALI: loss 11.07145 acc 11.07145\n",
            "epoch: 3 | TRAIN: loss 30.49952 acc 8.87852 | VALI: loss 11.2078 acc 11.2078\n",
            "epoch: 4 | TRAIN: loss 29.51088 acc 8.10987 | VALI: loss 11.40873 acc 11.40873\n",
            "epoch: 5 | TRAIN: loss 28.46396 acc 7.34457 | VALI: loss 11.75159 acc 11.75159\n",
            "epoch: 6 | TRAIN: loss 27.46095 acc 6.66943 | VALI: loss 12.2201 acc 12.2201\n",
            "epoch: 7 | TRAIN: loss 26.54703 acc 6.11153 | VALI: loss 12.76121 acc 12.76121\n",
            "epoch: 8 | TRAIN: loss 25.72584 acc 5.6609 | VALI: loss 13.33037 acc 13.33037\n",
            "epoch: 9 | TRAIN: loss 24.98421 acc 5.29585 | VALI: loss 13.89902 acc 13.89902\n",
            "Parameter containing:\n",
            "tensor([[[1.1574, 0.7882]]], requires_grad=True)\n",
            "tensor(0.4631, grad_fn=<MeanBackward0>)\n",
            "(12.433172464080679, 12.433172464080679)\n",
            "epoch: 0 | TRAIN: loss 5377.93284 acc 3825.21689 | VALI: loss 131.39737 acc 131.39737\n",
            "epoch: 1 | TRAIN: loss 1989.98379 acc 89.76189 | VALI: loss 90.26183 acc 90.26183\n",
            "epoch: 2 | TRAIN: loss 1499.15337 acc 68.38276 | VALI: loss 70.47616 acc 70.47616\n",
            "epoch: 3 | TRAIN: loss 1194.79719 acc 49.22669 | VALI: loss 54.18545 acc 54.18545\n",
            "epoch: 4 | TRAIN: loss 988.00933 acc 36.88578 | VALI: loss 43.80968 acc 43.80968\n",
            "epoch: 5 | TRAIN: loss 835.98093 acc 29.08034 | VALI: loss 37.02478 acc 37.02478\n",
            "epoch: 6 | TRAIN: loss 718.91273 acc 23.94646 | VALI: loss 32.39685 acc 32.39685\n",
            "epoch: 7 | TRAIN: loss 626.5481 acc 20.45357 | VALI: loss 29.11232 acc 29.11232\n",
            "epoch: 8 | TRAIN: loss 552.79748 acc 18.0135 | VALI: loss 26.68511 acc 26.68511\n",
            "epoch: 9 | TRAIN: loss 493.5253 acc 16.2677 | VALI: loss 24.81783 acc 24.81783\n",
            "Parameter containing:\n",
            "tensor([[[0.2088, 0.2370]]], requires_grad=True)\n",
            "tensor(1.9473, grad_fn=<MeanBackward0>)\n",
            "(24.60731953885878, 24.60731953885878)\n",
            "epoch: 0 | TRAIN: loss 6417.87109 acc 6254.38484 | VALI: loss 1892.30045 acc 1892.30045\n",
            "epoch: 1 | TRAIN: loss 628.28119 acc 415.74719 | VALI: loss 8.63625 acc 8.63625\n",
            "epoch: 2 | TRAIN: loss 1.25935 acc 1.0354 | VALI: loss 0.65369 acc 0.65369\n",
            "epoch: 3 | TRAIN: loss 0.62432 acc 0.40601 | VALI: loss 0.5874 acc 0.5874\n",
            "epoch: 4 | TRAIN: loss 0.56796 acc 0.32293 | VALI: loss 0.49882 acc 0.49882\n",
            "epoch: 5 | TRAIN: loss 0.47691 acc 0.23093 | VALI: loss 0.41048 acc 0.41048\n",
            "epoch: 6 | TRAIN: loss 0.35653 acc 0.14846 | VALI: loss 0.32303 acc 0.32303\n",
            "epoch: 7 | TRAIN: loss 0.31927 acc 0.09044 | VALI: loss 0.25178 acc 0.25178\n",
            "epoch: 8 | TRAIN: loss 0.26616 acc 0.05768 | VALI: loss 0.19761 acc 0.19761\n",
            "epoch: 9 | TRAIN: loss 0.26887 acc 0.04288 | VALI: loss 0.15815 acc 0.15815\n",
            "Parameter containing:\n",
            "tensor([[[ 1.6589, -1.6589]]], requires_grad=True)\n",
            "tensor(0.6067, grad_fn=<MeanBackward0>)\n",
            "(0.16264533628558275, 0.16264533628558275)\n",
            "epoch: 0 | TRAIN: loss 6541.7032 acc 6328.98145 | VALI: loss 1789.53448 acc 1789.53448\n",
            "epoch: 1 | TRAIN: loss 628.20559 acc 383.00349 | VALI: loss 8.26792 acc 8.26792\n",
            "epoch: 2 | TRAIN: loss 1.26957 acc 1.04357 | VALI: loss 0.63472 acc 0.63472\n",
            "epoch: 3 | TRAIN: loss 0.64493 acc 0.43862 | VALI: loss 0.57257 acc 0.57257\n",
            "epoch: 4 | TRAIN: loss 0.59835 acc 0.35002 | VALI: loss 0.48206 acc 0.48206\n",
            "epoch: 5 | TRAIN: loss 0.50856 acc 0.25009 | VALI: loss 0.39486 acc 0.39486\n",
            "epoch: 6 | TRAIN: loss 0.39762 acc 0.15988 | VALI: loss 0.30583 acc 0.30583\n",
            "epoch: 7 | TRAIN: loss 0.27255 acc 0.09699 | VALI: loss 0.24314 acc 0.24314\n",
            "epoch: 8 | TRAIN: loss 0.26818 acc 0.06287 | VALI: loss 0.20258 acc 0.20258\n",
            "epoch: 9 | TRAIN: loss 0.23592 acc 0.04802 | VALI: loss 0.18128 acc 0.18128\n",
            "Parameter containing:\n",
            "tensor([[[-1.5313,  1.5315]]], requires_grad=True)\n",
            "tensor(0.6577, grad_fn=<MeanBackward0>)\n",
            "(0.161906902703573, 0.161906902703573)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs44ze96kHZZ",
        "colab_type": "text"
      },
      "source": [
        "<b>2G)</b> With the above choice of regularization parameter, what is the mean square error of the best network that you find on the test data? Try a few trials (5 or more) for each data test and report the value of the best network. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAN0k9wylOmz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### We expect the training to be easier when there are fewer parameters to learn. Consider images of size $128\\times 1$ for the rest of the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnktFwXRkKNF",
        "colab_type": "text"
      },
      "source": [
        "<b>2H)</b> Instead of resorting to regularization again, we may instead find a way to reduce the number of parameters. What additional layer can you add to the output of the convolution layer to reduce the number of parameters to be learned without losing any relevant information?\n",
        "\n",
        "<a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2020_Spring/courseware/Week8/week8_homework/\">Refer to HW8 on MITx.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXgOqKtRkNRP",
        "colab_type": "text"
      },
      "source": [
        "<b>2I)</b> Add the layer you suggested above to your network and run some tests with data sets of size 1000 on $128\\times 1$ images.  How many parameters are left to learn with the new structure?\n",
        "\n",
        "You can find the appropriate documentations for the new types of modules mentioned in the previous problem here:\n",
        "\n",
        "<a href=\"https://pytorch.org/docs/stable/nn.html#dropout\">Dropout</a>\n",
        "\n",
        "<a href=\"https://pytorch.org/docs/stable/nn.html#maxpool1d\">MaxPool1d</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQGlJLxI__4A",
        "colab_type": "text"
      },
      "source": [
        "# 3) MNIST (Digit Classification)\n",
        "\n",
        "In this section, we'll be looking at the MNIST data set seen already in problem 2. This time, we look at the *complete* MNIST problem where our networks will take an image of *any* digit from $0-9$ as input (recall that problem 2 only looked at digits $0$ and $1$) and try to predict that digit. Note that in general, an image is described as a two-dimensional array of pixels. Here, the image is a <a href=\"https://en.wikipedia.org/wiki/Grayscale\">grayscale</a> image, so each pixel is represented by only one integer value, in the range $0$ to $255$ (compared to RGB images where each pixel is represented by three integer values, encoding intensity levels in red, green, and blue color channels).\n",
        "\n",
        "Also, we will now use out-of-the-box neural network implementations using PyTorch. State-of-the-art systems have error rates of less than 0.5% percent on this data set (see <a href=\"http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354\">this list</a>).  We'll be happy with an error rate less than 2% since we don't have all year...\n",
        "\n",
        "You can access the MNIST data for this problem using:\n",
        "<br><code>train, validation = get_MNIST_data()</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrIFeBFNWu38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shifted(X, shift):\n",
        "    n = X.shape[0]\n",
        "    m = X.shape[1]\n",
        "    size = m + shift\n",
        "    X_sh = np.zeros((n, size, size))\n",
        "    plt.ion()\n",
        "    for i in range(n):\n",
        "        sh1 = np.random.randint(shift)\n",
        "        sh2 = np.random.randint(shift)\n",
        "        X_sh[i, sh1:sh1+m, sh2:sh2+m] = X[i, :, :]\n",
        "        # If you want to see the shifts, uncomment\n",
        "        #plt.figure(1); plt.imshow(X[i])\n",
        "        #plt.figure(2); plt.imshow(X_sh[i])\n",
        "        #plt.show()\n",
        "        #input('Go?')\n",
        "    return X_sh\n",
        "  \n",
        "def get_MNIST_data(shift=0):\n",
        "    train = MNIST(root='./mnist_data', train=True, download=True, transform=None)\n",
        "    val = MNIST(root='./mnist_data', train=False, download=True, transform=None)\n",
        "    (X_train, y1), (X_val, y2) = (train.data.numpy(), train.targets.numpy()), \\\n",
        "                                  (val.data.numpy(), val.targets.numpy())\n",
        "    if shift:\n",
        "        X_train = shifted(X_train, shift)\n",
        "        X_val = shifted(X_val, shift)\n",
        "    return (X_train, y1), (X_val, y2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZvlHZS3WjRA",
        "colab_type": "text"
      },
      "source": [
        "You can run the fully connected MNIST model, using:\n",
        "<br><code>run_pytorch_fc_mnist(train, validation, layers, epochs)</code>\n",
        "\n",
        "And, you can run the CNN MNIST test, using:\n",
        "<br><code>run_pytorch_cnn_mnist(train, validation, layers, epochs)</code>\n",
        "\n",
        "You will need to design your own `layers` to feed to `run_pytorch_fc_mnist` and `run_pytorch_cnn_mnist`, which will be different than the ones specified by `archs()`. For instance, `layers=[Linear(in_features=64, out_features=4)]` defines a single layer with 64 inputs and 4 output units.\n",
        "Note that the training procedure, uses <a href=\"https://pytorch.org/docs/stable/nn.html#crossentropyloss\">PyTorch's CrossEntropyLoss</a>, which handles the softmax activations for you, so adding a softmax layer to the end of your network is not necessary and will produce undesired results.\n",
        "Also, we advise you to use the option `verbose=True` when unsure about the progress made during training of your models.\n",
        "#### **IMPORTANT:** For this and subsequent questions, use the PyTorch implementation of modules. For example, for a linear layer, use <code>nn.Linear(...)</code>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx1jt6P9AUk1",
        "colab_type": "text"
      },
      "source": [
        "<b> 3A)</b> Look at the code and indicate what the difference is between <code>run_pytorch_fc_mnist</code> and <code>run_pytorch_cnn_mnist</code>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5moSfb7CcXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_deterministic():\n",
        "    torch.manual_seed(10)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(10)\n",
        "\n",
        "\n",
        "def weight_reset(l):\n",
        "    if isinstance(l, Conv2d) or isinstance(l, Linear):\n",
        "        l.reset_parameters()\n",
        "\n",
        "def run_pytorch_fc_mnist(train, test, layers, epochs, verbose=True, trials=1, deterministic=True):\n",
        "    '''\n",
        "    train, test = input data\n",
        "    layers = list of PyTorch layers, e.g. [Linear(in_features=784, out_features=10)]\n",
        "    epochs = number of epochs to run the model for each training trial\n",
        "    trials = number of evaluation trials, resetting weights before each trial\n",
        "    '''\n",
        "    if deterministic:\n",
        "        make_deterministic()\n",
        "    (X_train, y1), (X_val, y2) = train, test\n",
        "    # Flatten the images\n",
        "    m = X_train.shape[1]\n",
        "    X_train = X_train.reshape((X_train.shape[0], m * m))\n",
        "    X_val = X_val.reshape((X_val.shape[0], m * m))\n",
        "\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        for l in layers:\n",
        "            weight_reset(l)\n",
        "        # Make Dataset Iterables\n",
        "        train_iter, val_iter = make_iter(X_train, y1, batch_size=32), make_iter(X_val, y2, batch_size=32)\n",
        "        # Run the model\n",
        "        model, vacc, tacc = \\\n",
        "            run_pytorch(train_iter, val_iter, None, layers, epochs, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "    if val_acc:\n",
        "        print(\"\\nAvg. validation accuracy:\" + str(val_acc / trials))\n",
        "    if test_acc:\n",
        "        print(\"\\nAvg. test accuracy:\" + str(test_acc / trials))\n",
        "\n",
        "\n",
        "def run_pytorch_cnn_mnist(train, test, layers, epochs, verbose=True, trials=1, deterministic=True):\n",
        "    if deterministic:\n",
        "        make_deterministic()\n",
        "    # Load the dataset\n",
        "    (X_train, y1), (X_val, y2) = train, test\n",
        "    # Add a final dimension indicating the number of channels (only 1 here)\n",
        "    m = X_train.shape[1]\n",
        "    X_train = X_train.reshape((X_train.shape[0], 1, m, m))\n",
        "    X_val = X_val.reshape((X_val.shape[0], 1, m, m))\n",
        "\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        for l in layers:\n",
        "            weight_reset(l)\n",
        "        # Make Dataset Iterables\n",
        "        train_iter, val_iter = make_iter(X_train, y1, batch_size=32), make_iter(X_val, y2, batch_size=32)\n",
        "        # Run the model\n",
        "        model, vacc, tacc = \\\n",
        "            run_pytorch(train_iter, val_iter, None, layers, epochs, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "    if val_acc:\n",
        "        print(\"\\nAvg. validation accuracy:\" + str(val_acc / trials))\n",
        "    if test_acc:\n",
        "        print(\"\\nAvg. test accuracy:\" + str(test_acc / trials))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sGfqAbICbmE",
        "colab_type": "text"
      },
      "source": [
        "<b> 3B)</b> Using one epoch of training, what is the accuracy of a network **with no hidden units** (using the <code>run_pytorch_fc_mnist</code> method) on this data?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1VAxZ17DtPQ",
        "colab_type": "code",
        "outputId": "7e7f0819-a344-4ca1-b7c5-eb3279eab688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train, validation = get_MNIST_data()\n",
        "layers = [nn.Linear(in_features=28*28, out_features=10)]\n",
        "run_pytorch_fc_mnist(train, validation, layers, 1, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 0.19019 acc 0.85258 | VALI: loss 0.23529 acc 0.8536\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.8536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsQ31e1lDE6u",
        "colab_type": "text"
      },
      "source": [
        "<b> 3C)</b> Now, linearly scale the data so that the pixel values are between 0 and 1 and repeat your test with the original layer. What is the accuracy now?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b-RcZu1EPaj",
        "colab_type": "code",
        "outputId": "ce467bed-ef38-44a9-807a-c8146b6417c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train, validation = get_MNIST_data()\n",
        "\n",
        "# Scale the images\n",
        "train = train[0] / 255, train[1]\n",
        "validation = validation[0] / 255, validation[1]\n",
        "\n",
        "run_pytorch_fc_mnist(train, validation, layers, epochs=1, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 0.00766 acc 0.9326 | VALI: loss 0.00858 acc 0.924\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A__3x6eyEIFn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrYGfcOLEr0f",
        "colab_type": "text"
      },
      "source": [
        "### Important: <b>Always scale the data like in 3C) for subsequent problems.</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHoyqJdqDFH5",
        "colab_type": "text"
      },
      "source": [
        "<b> 3E)</b> Using this same architecture, what is the accuracy after the 1st, 5th, 10th, and 15th epochs? Note that this colab notebook 0-indexes epoch output. We're looking for the first, fifth, tenth, and fifteenth number outputted by <code>run_pytorch_fc_mnist(train, validation, layers, 15, verbose=True)</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8PlbWS_EwTv",
        "colab_type": "code",
        "outputId": "cdeb2a56-9def-4c97-872c-4c45b0ace38b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "run_pytorch_fc_mnist(train, validation, layers, epochs=15, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 0.00764 acc 0.93283 | VALI: loss 0.00861 acc 0.9242\n",
            "epoch: 1 | TRAIN: loss 0.00761 acc 0.93343 | VALI: loss 0.00862 acc 0.924\n",
            "epoch: 2 | TRAIN: loss 0.00759 acc 0.93355 | VALI: loss 0.00864 acc 0.9236\n",
            "epoch: 3 | TRAIN: loss 0.00757 acc 0.93363 | VALI: loss 0.00866 acc 0.9237\n",
            "epoch: 4 | TRAIN: loss 0.00755 acc 0.93373 | VALI: loss 0.00867 acc 0.9236\n",
            "epoch: 5 | TRAIN: loss 0.00753 acc 0.93388 | VALI: loss 0.00869 acc 0.9236\n",
            "epoch: 6 | TRAIN: loss 0.00751 acc 0.93415 | VALI: loss 0.00871 acc 0.9234\n",
            "epoch: 7 | TRAIN: loss 0.00749 acc 0.93418 | VALI: loss 0.00872 acc 0.9233\n",
            "epoch: 8 | TRAIN: loss 0.00748 acc 0.9342 | VALI: loss 0.00874 acc 0.9232\n",
            "epoch: 9 | TRAIN: loss 0.00746 acc 0.93425 | VALI: loss 0.00876 acc 0.9231\n",
            "epoch: 10 | TRAIN: loss 0.00745 acc 0.93447 | VALI: loss 0.00877 acc 0.9234\n",
            "epoch: 11 | TRAIN: loss 0.00743 acc 0.93452 | VALI: loss 0.00879 acc 0.9229\n",
            "epoch: 12 | TRAIN: loss 0.00742 acc 0.9347 | VALI: loss 0.00881 acc 0.9229\n",
            "epoch: 13 | TRAIN: loss 0.00741 acc 0.93488 | VALI: loss 0.00882 acc 0.9232\n",
            "epoch: 14 | TRAIN: loss 0.0074 acc 0.93493 | VALI: loss 0.00884 acc 0.9234\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9234333333333332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCPTuz-tDFTg",
        "colab_type": "text"
      },
      "source": [
        "<b> 3H)</b> Using one epoch of training, try a single hidden layer, followed by a ReLU activation layer before the final output layer, and gradually increase the units; specifically, try (128, 256, 512, 1024) units and observe the results.  What are the accuracies?\n",
        "To define a ReLU layer in pytorch simply use <code>ReLU()</code>.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmpbP_VHFoh1",
        "colab_type": "code",
        "outputId": "f7792257-ebab-4bf9-b359-c9ad0a70a0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "for num in [128,256,512,1024]:\n",
        "    print(f\"Using {num} hidden neurons\")\n",
        "    layers = [nn.Linear(in_features=28*28, out_features=num), nn.ReLU(), nn.Linear(in_features=num, out_features=10)]\n",
        "    run_pytorch_fc_mnist(train, validation, layers, 1, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 128 hidden neurons\n",
            "epoch: 0 | TRAIN: loss 0.0096 acc 0.91493 | VALI: loss 0.00546 acc 0.9478\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9478\n",
            "Using 256 hidden neurons\n",
            "epoch: 0 | TRAIN: loss 0.0083 acc 0.92433 | VALI: loss 0.00464 acc 0.953\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.953\n",
            "Using 512 hidden neurons\n",
            "epoch: 0 | TRAIN: loss 0.00725 acc 0.93253 | VALI: loss 0.00414 acc 0.9581\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9581\n",
            "Using 1024 hidden neurons\n",
            "epoch: 0 | TRAIN: loss 0.0065 acc 0.93748 | VALI: loss 0.00387 acc 0.9596\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEM5mZi5DFYS",
        "colab_type": "text"
      },
      "source": [
        "<b> 3I)</b> Now, try a network with two hidden layers:\n",
        "<ul>\n",
        "  <li>A fully connected layer with 512 hidden units\n",
        "  <li> A ReLU activation layer\n",
        "  <li> A fully connected layer with 256 hidden units\n",
        "  <li> A ReLU activation layer\n",
        "  <li> A fully-connected layer with 10 output units\n",
        "    \n",
        "What is the accuracy?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cwp1VR7F06q",
        "colab_type": "code",
        "outputId": "56c233ed-c72f-4546-e5a4-993973484329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "layers = [\n",
        "    nn.Linear(in_features=28*28, out_features=512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=512, out_features=256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=256, out_features=10)\n",
        "]\n",
        "\n",
        "run_pytorch_fc_mnist(train, validation, layers, 1, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 0.00681 acc 0.93478 | VALI: loss 0.00417 acc 0.9579\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmnNUT2nDFdi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<b> 3J)</b> Build a convolutional network with the following structure:\n",
        "\n",
        "<ul>\n",
        "<li> A convolutional layer with 32 filters of size 3  3\n",
        "<li> A ReLU activation layer\n",
        "<li> A max pooling layer with size 2  2\n",
        "<li> A convolutional layer with 64 filters of size 3  3\n",
        "<li> A ReLU activation layer\n",
        "<li> A max pooling layer with size 2  2\n",
        "<li> A flatten layer (<b>will flatten to size 1600</b>; try to figure out why!)\n",
        "<li> A fully connected layer with 128 units \n",
        "<li> A ReLU activation layer\n",
        "<li> A dropout layer with drop probability 0.5\n",
        "<li> A fully-connected layer with 10 output units\n",
        "</ul>\n",
        "To define Convolutional and max pooling layers in PyTorch use the following syntax:\n",
        "<code> c = Conv2d(in_channels=i, out_channels=o, kernel_size=filter_size); m = MaxPool2d(kernel_size=filter_size) </code>, where <code> i </code> and <code> o</code> are integers and <code>filter_size</code> can either be an integer (for square filters) or a tuple (for non-square filters i.e. (2, 3) for 2x3 filter).\n",
        "Train it on MNIST for one epoch, using <code>run_pytorch_cnn_mnist</code> (this may take a little while).  What is the accuracy on the validation set?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-sPW8xKF7EZ",
        "colab_type": "code",
        "outputId": "a31d39c1-a451-4115-b340-912e603c4982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "layers = [\n",
        "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(in_features=1600, out_features=128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(in_features=128, out_features=10)\n",
        "]\n",
        "\n",
        "run_pytorch_cnn_mnist(train, validation, layers, 1, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 0.0077 acc 0.9249 | VALI: loss 0.00212 acc 0.9786\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjmqgGvIDFiS",
        "colab_type": "text"
      },
      "source": [
        "<b> 3K)</b> Now, let's compare the performance of a fully connected model and a CNN on data where the characters have been shifted randomly so that they are no longer centered.  \n",
        "\n",
        "You can build such a data set by calling: <code>train_20, validation_20 = get_MNIST_data(shift=20)</code>. Remember to scale it appropriately.\n",
        "\n",
        "<b>Note that each image is now 48x48, so you will need to change your layer definitions (size after Flatten will be 6400)</b>.\n",
        "Run your two-hidden-layer FC architecture from above (problem 3I) on this data and then run the CNN architecture from above (problem 3J), both for one epoch. Report your results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvfiyrN9Gf7X",
        "colab_type": "code",
        "outputId": "b0f04e06-3a5f-4248-c06b-d47538fc40cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "train_20, validation_20 = get_MNIST_data(shift=20)\n",
        "\n",
        "# Scale the images\n",
        "train_20 = (train_20[0] / 255, train_20[1])\n",
        "validation_20 = (validation_20[0] / 255, validation_20[1])\n",
        "\n",
        "# Fully Connected\n",
        "layers_fc = [\n",
        "    nn.Linear(in_features=48*48, out_features=512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=512, out_features=256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=256, out_features=10)\n",
        "]\n",
        "\n",
        "run_pytorch_fc_mnist(train_20, validation_20, layers_fc, 1, verbose=True)\n",
        "\n",
        "# CNN\n",
        "layers_cnn = [\n",
        "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(in_features=6400, out_features=128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(in_features=128, out_features=10)\n",
        "]\n",
        "\n",
        "run_pytorch_cnn_mnist(train_20, validation_20, layers_cnn, 1, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 0.02367 acc 0.74463 | VALI: loss 0.01344 acc 0.8601\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.8601\n",
            "epoch: 0 | TRAIN: loss 0.0273 acc 0.70153 | VALI: loss 0.00876 acc 0.9247\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL4ZfEWIBB4B",
        "colab_type": "text"
      },
      "source": [
        "# 4. Raining Cats and Dogs\n",
        "\n",
        "In this problem, we are going to explore how a model trained on a particular dataset behaves in the general population. We will use the following functions (and [generator](https://wiki.python.org/moin/Generators))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7JiISTWFi0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_images(directory, imdir='./'):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    for i in os.listdir(os.path.join(imdir, directory)):\n",
        "        img = resize(imread(os.path.join(imdir, directory, i)), (224, 224), anti_aliasing=True)\n",
        "        imgs.append(np.moveaxis(img, 2, 0))\n",
        "        if 'cat' in i:\n",
        "            labels.append(0)\n",
        "        else:\n",
        "            labels.append(1)\n",
        "    imgs = np.array(imgs)\n",
        "    return imgs, labels\n",
        "\n",
        "def data_gen(train_images, train_labels, batch_size, eval=True):\n",
        "    all_idxs = np.arange(len(train_labels))\n",
        "    idxs = np.random.shuffle(all_idxs)\n",
        "    i = 0\n",
        "    while i * batch_size + batch_size < train_images.shape[0]:\n",
        "        samples = train_images[i*batch_size: (i+1)*batch_size]\n",
        "        sample_labels = train_labels[i*batch_size: (i+1)*batch_size]\n",
        "        i += 1\n",
        "        \n",
        "        yield torch.tensor(samples, dtype=torch.float), torch.tensor(sample_labels, dtype=torch.long)\n",
        "    if eval:\n",
        "        samples = train_images[(i)*batch_size:]\n",
        "        sample_labels = train_labels[(i)*batch_size:]\n",
        "        yield torch.tensor(samples, dtype=torch.float), torch.tensor(sample_labels, dtype=torch.long)\n",
        "def postproc_output(out):\n",
        "  sm = torch.nn.Softmax(dim=1)\n",
        "  return sm(out).detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-07d6KNTFjB_",
        "colab_type": "text"
      },
      "source": [
        "**4A)** Write code to evaluate the model on each of the three test sets, following this pseudocode:\n",
        "<ol>\n",
        "    <li> Load model \n",
        "    <ul>\n",
        "      <li> <tt> squeezenet_trained_cats_v_dogs.pt</tt> contains the <tt>state_dict</tt> of the model\n",
        "        <li> You will need to instantiate the model architecture first by running:\n",
        "          <tt>model = torchvision.models.squeezenet1_1(num_classes=2)</tt>\n",
        "        <li> Then use <a href=\"https://pytorch.org/docs/stable/nn.html?highlight=load_state_dict#torch.nn.Module.load_state_dict\"><tt> model.load_state_dict</tt> </a>; make sure to use the parameter <tt>map_location=torch.device('cpu')</tt> when you use <tt>torch.load</tt>. It might also be helpful to read about the general workflow of <a href=\"https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference\"> saving and loading trained models in Pytorch </a>\n",
        "      <li> Make sure that you are in evaluation model by using <tt> model.eval() </tt>\n",
        "    </ul>\n",
        "  <li> Load data from <tt>data_path</tt> [Use our function <tt>load_images</tt>]\n",
        "  <li> For each batch of data [Use our generator <tt>data_gen</tt>; note that the batch size doesn't really matter here except to keep from having to multiply matrices that are too large. For ease of implementation we suggest <tt>batch_size=1</tt>]:\n",
        "    <ul>\n",
        "      <li> Pass the batch of <tt>data</tt> through the model using <tt>model(data)</tt>\n",
        "        <li> Convert the predictions of the model to guesses (after softmax) [use our function <tt>postproc_output</tt>]\n",
        "      <li> Compare the guesses to the actual <tt>labels</tt>\n",
        "    </ul>\n",
        "    <li> Total the accuracy\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIG9E0NgBjo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model_path, data_path):\n",
        "    # Load model\n",
        "    model = torchvision.models.squeezenet1_1(num_classes=2)\n",
        "    model.load_state_dict(\n",
        "        torch.load(model_path, map_location=torch.device('cpu')),\n",
        "    )\n",
        "    model.eval()\n",
        "\n",
        "    # Load data\n",
        "    batch_size = 1\n",
        "    imgs, labels = load_images(data_path)\n",
        "    data_generator = data_gen(imgs, labels, batch_size=1)\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    \n",
        "    # Iterate through data, label in data generator\n",
        "    for img, label in data_generator:\n",
        "        pred = model(img)\n",
        "        pred = postproc_output(pred)\n",
        "        pred = np.argmax(pred, axis=1)\n",
        "        correct += pred.item() == label.item()\n",
        "        total +=1\n",
        "    \n",
        "    print(f\"Correct prediction {correct / float(total)} of the time.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "111yzeXLB2lH",
        "colab_type": "text"
      },
      "source": [
        "Calculate the performance on each of the test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtF_dnjXid6f",
        "colab_type": "code",
        "outputId": "02dd8822-8f4d-4c68-d63c-edeab6d1bad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "model_path = 'squeezenet_trained_cats_v_dogs.pt'\n",
        "for data_path in ['test1', 'test2', 'test3']:\n",
        "    print(f\"Running inference for test set '{data_path}'.\")\n",
        "    evaluate_model(model_path, data_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running inference for test set 'test1'.\n",
            "Correct prediction 0.98 of the time.\n",
            "Running inference for test set 'test2'.\n",
            "Correct prediction 0.49 of the time.\n",
            "Running inference for test set 'test3'.\n",
            "Correct prediction 0.055 of the time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUGGqxCOIYCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}